{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3171e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 22:00:02,840 - INFO - Successfully loaded 1827 chunks.\n",
      "2025-06-26 22:00:02,841 - INFO - Generating embeddings for chunks...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded 1827 chunks from transcripts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 22:00:03,321 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-06-26 22:00:03,996 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-06-26 22:00:04,539 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-06-26 22:00:04,969 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-06-26 22:00:05,660 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-06-26 22:00:06,395 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-06-26 22:00:07,089 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-06-26 22:00:07,809 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-06-26 22:00:08,470 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-06-26 22:00:09,171 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-06-26 22:00:09,819 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-06-26 22:00:10,446 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-06-26 22:00:10,844 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-06-26 22:00:11,403 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-06-26 22:00:12,228 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-06-26 22:00:12,895 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-06-26 22:00:13,604 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-06-26 22:00:14,300 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-06-26 22:00:14,970 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-06-26 22:00:15,706 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-06-26 22:00:16,274 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-06-26 22:00:16,889 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-06-26 22:00:17,448 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-06-26 22:00:18,141 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-06-26 22:00:19,262 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-06-26 22:00:19,766 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-06-26 22:00:20,524 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-06-26 22:00:21,594 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-06-26 22:00:22,475 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-06-26 22:00:23,104 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-06-26 22:00:23,504 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-06-26 22:00:24,205 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-06-26 22:00:24,875 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-06-26 22:00:25,545 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-06-26 22:00:26,207 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-06-26 22:00:26,907 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-06-26 22:00:27,584 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-06-26 22:00:27,982 - INFO - Generated 1827 chunk embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to Financial Literacy Chatbot (short pipeline demo). Type 'exit' to quit.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 22:00:41,047 - INFO - Processing query: 'can you summerize khan academy videos?'\n",
      "2025-06-26 22:00:41,498 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-06-26 22:00:41,694 - INFO - \n",
      "ðŸ”Ž Retrieved top 5 chunks (indices: [986, 836, 1245, 1739, 185]):\n",
      "<DOCUMENT_START id=986>\n",
      " like you said obviously YouTube jumps to mind I suspect that's on this list somewhere but you can learn essentially anything you need in the entire world on YouTube we talked about the library there are free courses the the moocs the what are they the massively open online courses whatever they stand for there's Khan Academy there's you know like you said sure personal finance books are great but you know you kind of run to the end of that pretty quickly but you can then get into psychology lik\n",
      "<DOCUMENT_END>\n",
      "\n",
      "<DOCUMENT_START id=836>\n",
      "are likely to be which is why you should now watch this video here where i explain how you can start to predict them much more reliably i'll see you there\n",
      "<DOCUMENT_END>\n",
      "\n",
      "<DOCUMENT_START id=1245>\n",
      "e most important questions in your personal life these arbitrary goals are preventing you from putting in the real work to find out when you can actually afford to retire and if you want to learn how to answer this question properly in the description of this video you will find links to three videos that take you through the step-by-step process us that I use with my clients number five losing your best years kicking the can down the road and working for just one more year may not seem like tha\n",
      "<DOCUMENT_END>\n",
      "\n",
      "<DOCUMENT_START id=1739>\n",
      "(jaunty music) - [Narrator] Khan Academy proudly presents \"A Tale of Two Credit Scores.\" This is Janae.\n",
      "- [Janae] Hi. - [Narrator] And this is Bob. - [Bob] Good morning, narrator. - [Narrator] Hi, Bob. These two charming\n",
      "characters are coworkers, each with the same job and salary. They both need a vehicle, so they've traveled to this\n",
      "used car dealership together so they can each buy a new ride. - [Janae] I'm all about this model. It's got everything I need. It's got the all-wheel drive,\n",
      "it's got\n",
      "<DOCUMENT_END>\n",
      "\n",
      "<DOCUMENT_START id=185>\n",
      "try and maximize the income that you can draw from it which is why you should now watch this video here where i talk you through the three simple steps that you need to follow to maximize your income in retirement i'll see you there\n",
      "<DOCUMENT_END>\n",
      "\n",
      "2025-06-26 22:00:41,695 - INFO - Sending query to LLM...\n",
      "2025-06-26 22:00:42,817 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-26 22:00:42,820 - INFO - LLM response received.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ’¬ Answer:\n",
      "I apologize, but I don't have enough information in my knowledge base to answer that question based on the provided context.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 22:01:30,547 - INFO - Processing query: 'What are the tax implications of withdrawing from a 401(k) before age 59Â½?'\n",
      "2025-06-26 22:01:30,895 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-06-26 22:01:31,069 - INFO - \n",
      "ðŸ”Ž Retrieved top 5 chunks (indices: [304, 181, 1708, 1341, 1700]):\n",
      "<DOCUMENT_START id=304>\n",
      "can be invested and grows free of income and capital gains tax the only downside to pensions is that you cannot access them until your 55th birthday but when you reach 55 you get access to the funds inside your pension and you can take 25 percent of the value as a tax free lump sum and the other 75 can be drawn down and it's taxed as if it was your own income alternatively if you don't take the 25 tax-free lump sum straight away as soon as you hit 55 you can choose to do it gradually over time s\n",
      "<DOCUMENT_END>\n",
      "\n",
      "<DOCUMENT_START id=181>\n",
      "ns tax you can then draw 25 of that out tax-free and the rest will be taxed at your marginal income tax rate in retirement which is likely to be less than you're paying now the only downside literally the only downside to a pension is that you can't access them until you're 55 but you're already there so that's not a problem at all on that original video that i made talking about how you should be investing in your 20s and 30s i got a lot of comments from people asking for specific content about\n",
      "<DOCUMENT_END>\n",
      "\n",
      "<DOCUMENT_START id=1708>\n",
      "t's really hard to make the decision to forego taking advantage of that today. Look, there just putting a little disclaimer out there. Obviously, there's factors that influence this. If you're somebody who's under 30 years of age, your age and the years, the decades of taxfree growth is going to obviously push that needle towards Roth. Um, even if your tax bracket's going up. But then there's other people I think about we've had 60s something year olds who have come to us said hey Brian I know t\n",
      "<DOCUMENT_END>\n",
      "\n",
      "<DOCUMENT_START id=1341>\n",
      "Â \n",
      "thing that you have to think about is was theÂ Â  person who you inherited the IRA from were theyÂ \n",
      "already receiving RMDs So were they over the ageÂ Â  of 73 That sends you down one path If theyÂ \n",
      "were not that puts you on a different pathÂ Â  If they are already receiving RMDs then you haveÂ \n",
      "to follow a different schedule And again this isÂ Â  something you can do on your own You can use someÂ \n",
      "AI tools if you want to double check your mathÂ Â  If you want to hire a CPA or a financial plannerÂ \n",
      "this is a \n",
      "<DOCUMENT_END>\n",
      "\n",
      "<DOCUMENT_START id=1700>\n",
      "eduction when you make the contribution. They do grow in a tax deferred meaning that every year as it's getting interest as it's getting dividends as it's getting capital appreciation. The government's not going to be taxing on it. But unfortunately at some point in the future, usually your required minimum distribution age around 75, the government is going to start making you pull this money out to pay taxes. So it's good. It's just not tax-free good like that that first bucket was. So the typ\n",
      "<DOCUMENT_END>\n",
      "\n",
      "2025-06-26 22:01:31,070 - INFO - Sending query to LLM...\n",
      "2025-06-26 22:01:32,120 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-26 22:01:32,125 - INFO - LLM response received.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ’¬ Answer:\n",
      "I apologize, but I don't have enough information in my knowledge base to answer that question based on the provided context.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 22:01:53,681 - INFO - Processing query: 'How do annuities work and when might they be a good option for retirement income?'\n",
      "2025-06-26 22:01:54,078 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-06-26 22:01:54,240 - INFO - \n",
      "ðŸ”Ž Retrieved top 5 chunks (indices: [1176, 1170, 1179, 1169, 1600]):\n",
      "<DOCUMENT_START id=1176>\n",
      "at I've built for my subscribers to visualize their retirement plans but if you are interested in trying this out for yourself again you can find a link to this down in the description with this strategy we've used an annuity provide an income at the start of retirement but as an alternative Tim could set aside part of their pension say a hundred thousand pounds to be invested with the intention of buying an annuity later on in life at say 75 to provide them with a guaranteed income for the rest\n",
      "<DOCUMENT_END>\n",
      "\n",
      "<DOCUMENT_START id=1170>\n",
      "so in effect they only need to plan for the average life expectancy of the group rather than all the way up to 100 it which enables them to pay out much higher starting sums to everybody annuities have been unpopular for a very long time because the rates that they pay are directly linked to interest rates but over the last year as rates have been rising annuity rates have shot up to levels not seen for 15 years so how could Tim use an annuity to give him more financial stability and enable him \n",
      "<DOCUMENT_END>\n",
      "\n",
      "<DOCUMENT_START id=1179>\n",
      "oncerned he already plans to use up all of these Assets in the pursuit of that objective what's more is that because he would have only annuitized part of their pensions there would still be plenty left over if they did happen to die early the reason why an annuity works so well for Tim is that it reduces the uncertainty of the future giving him one less thing to worry about but for that he has to sacrifice flexibility once that annuity is set up that's it he can't change it but for some people \n",
      "<DOCUMENT_END>\n",
      "\n",
      "<DOCUMENT_START id=1169>\n",
      "uarantee that income even if the insurer goes bust to provide that guaranteed income the insurer will invest in exactly the same type of bonds that you would do if you were trying to do this on your own so how then are they able to pay out a much higher level of income well given that annuities are made up of tens of thousands of members insurers can accurately predict What proportion of those will die each year and use the funds from those that die earlier to balance out those that live longer \n",
      "<DOCUMENT_END>\n",
      "\n",
      "<DOCUMENT_START id=1600>\n",
      "'re going to get income in your retirement. So it's good to start\n",
      "thinking about them early.\n",
      "<DOCUMENT_END>\n",
      "\n",
      "2025-06-26 22:01:54,241 - INFO - Sending query to LLM...\n",
      "2025-06-26 22:02:01,555 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-26 22:02:01,558 - INFO - LLM response received.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ’¬ Answer:\n",
      "Annuities work by providing a guaranteed income stream during retirement. When you purchase an annuity, you pay a lump sum or series of payments to an insurance company, which in turn provides you with regular payments for a specified period or for the rest of your life. The insurer is able to provide this guaranteed income by pooling the funds of many annuity holders and investing in bonds. They can pay out higher levels of income because they predict the average life expectancy of their group of annuitants and use funds from those who pass away earlier to pay those who live longer.\n",
      "\n",
      "Annuities might be a good option for retirement income if you are looking for financial stability and a guaranteed income, especially as interest rates rise, which can lead to higher annuity rates. They are particularly beneficial if you are concerned about outliving your savings, as they can provide income for life. However, it's important to note that once an annuity is set up, it lacks flexibility and cannot be changed. This can be a trade-off for the peace of mind that comes with a stable and predictable income.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# Assuming prepare_chunks.py is a separate file that you control\n",
    "from prepare_chunks import read_and_chunk_transcripts\n",
    "import logging\n",
    "\n",
    "# Configure logging for better debugging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Initialize OpenAI client. It's good practice to get API key from environment.\n",
    "# Ensure OPENAI_API_KEY is set in your environment variables.\n",
    "try:\n",
    "    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    if not client.api_key:\n",
    "        raise ValueError(\"OPENAI_API_KEY environment variable not set.\")\n",
    "except ValueError as e:\n",
    "    logging.error(f\"Configuration Error: {e}\")\n",
    "    # Exit or handle gracefully if API key is missing\n",
    "    exit(\"Exiting: OpenAI API key is missing. Please set OPENAI_API_KEY environment variable.\")\n",
    "\n",
    "\n",
    "# --- 1. Improve Chunking Strategy (Conceptual Change, implement in prepare_chunks.py) ---\n",
    "# The way you chunk transcripts is critical. If chunks are too small, they lack context.\n",
    "# If too large, they might contain irrelevant info diluting the similarity.\n",
    "# Suggestions for `prepare_chunks.py`:\n",
    "# A. Fixed size with overlap: e.g., 500 characters per chunk with 100 characters overlap.\n",
    "#    This helps maintain context across chunk boundaries.\n",
    "# B. Sentence-based chunking: Ensure chunks don't cut sentences in half.\n",
    "# C. Paragraph-based chunking: If transcripts are paragraphed, use paragraphs as chunks.\n",
    "# D. Recursive character text splitter (LangChain concept): Splits by paragraphs, then sentences, then words, etc.\n",
    "#    This is more advanced but often yields better results.\n",
    "# Make sure your `read_and_chunk_transcripts` function handles this effectively.\n",
    "try:\n",
    "    chunks = read_and_chunk_transcripts('transcripts/')\n",
    "    if not chunks:\n",
    "        logging.warning(\"No chunks read from 'transcripts/'. Ensure files exist and content is processed.\")\n",
    "        # Handle case where no chunks are loaded, e.g., exit or use dummy data\n",
    "        exit(\"Exiting: No transcript chunks found. Please check 'transcripts/' directory and prepare_chunks.py.\")\n",
    "    logging.info(f\"Successfully loaded {len(chunks)} chunks.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error reading and chunking transcripts: {e}\")\n",
    "    exit(\"Exiting: Failed to process transcripts. Check 'prepare_chunks.py' and 'transcripts/' directory.\")\n",
    "\n",
    "\n",
    "def embed_texts(texts, model=\"text-embedding-ada-002\", batch_size=50):\n",
    "    \"\"\"\n",
    "    Generates embeddings for a list of texts using OpenAI's embedding API.\n",
    "    Handles batching and includes basic error handling.\n",
    "    \"\"\"\n",
    "    if not texts:\n",
    "        return []\n",
    "\n",
    "    embeddings = []\n",
    "    # Add a retry mechanism for robustness\n",
    "    max_retries = 3\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = client.embeddings.create(\n",
    "                    model=model,\n",
    "                    input=batch\n",
    "                )\n",
    "                embeddings.extend([item.embedding for item in response.data])\n",
    "                break # Break out of retry loop on success\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Embedding API error on batch {i//batch_size + 1}, attempt {attempt + 1}: {e}\")\n",
    "                if attempt == max_retries - 1:\n",
    "                    logging.error(f\"Max retries reached for embedding batch {i//batch_size + 1}. Skipping batch.\")\n",
    "                    # Optionally, append dummy embeddings or raise the error\n",
    "                # Add a small delay before retrying\n",
    "                import time\n",
    "                time.sleep(1 + attempt * 2) # Exponential backoff\n",
    "    return embeddings\n",
    "\n",
    "logging.info(\"Generating embeddings for chunks...\")\n",
    "chunk_embeddings = embed_texts(chunks)\n",
    "if not chunk_embeddings:\n",
    "    logging.error(\"Failed to generate embeddings for chunks. Check API key and network.\")\n",
    "    exit(\"Exiting: No chunk embeddings generated.\")\n",
    "logging.info(f\"Generated {len(chunk_embeddings)} chunk embeddings.\")\n",
    "\n",
    "\n",
    "def answer_query(query, top_k=5, llm_model=\"gpt-4o\", temperature=0.5): # Increased temperature slightly\n",
    "    \"\"\"\n",
    "    Retrieves relevant chunks and uses an LLM to answer the query.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Processing query: '{query}'\")\n",
    "\n",
    "    if not query.strip():\n",
    "        return \"Please enter a valid question.\"\n",
    "\n",
    "    # Embed the query\n",
    "    query_embedding = embed_texts([query])\n",
    "    if not query_embedding:\n",
    "        logging.error(\"Failed to generate embedding for the query.\")\n",
    "        return \"Sorry, I couldn't process your question at the moment.\"\n",
    "    query_embedding = query_embedding[0]\n",
    "\n",
    "    # Calculate similarities and get top_k indices\n",
    "    similarities = cosine_similarity([query_embedding], chunk_embeddings)[0]\n",
    "    # Ensure top_k doesn't exceed available chunks\n",
    "    effective_top_k = min(top_k, len(chunks))\n",
    "    top_indices = np.argsort(similarities)[-effective_top_k:][::-1]\n",
    "\n",
    "    # Combine top chunks into a single context with clear formatting\n",
    "    # --- 2. Improve Context Formatting ---\n",
    "    # Give the LLM clear markers for context sections.\n",
    "    # Emphasize that these are retrieved documents.\n",
    "    context_parts = []\n",
    "    for i in top_indices:\n",
    "        # Include original index for debugging, and a clear separator\n",
    "        context_parts.append(f\"<DOCUMENT_START id={i}>\\n{chunks[i]}\\n<DOCUMENT_END>\")\n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "\n",
    "    logging.info(f\"\\nðŸ”Ž Retrieved top {effective_top_k} chunks (indices: {[int(i) for i in top_indices]}):\\n{context}\\n\")\n",
    "\n",
    "\n",
    "    # --- 3. Refine Prompt Engineering ---\n",
    "    # Make the prompt more directive, clearly define the AI's role and instructions.\n",
    "    # Guide it to synthesize information, and be more flexible about \"I don't know\".\n",
    "    prompt = f\"\"\"You are a helpful and knowledgeable financial literacy assistant. Your task is to answer the user's question ONLY based on the information provided in the following retrieved documents.\n",
    "\n",
    "**Instructions:**\n",
    "- Read the provided documents carefully.\n",
    "- Synthesize the information to answer the question concisely and accurately.\n",
    "- If the question cannot be answered using *only* the provided documents, or if the documents do not contain enough relevant information, state clearly: \"I apologize, but I don't have enough information in my knowledge base to answer that question based on the provided context.\" Do NOT make up information.\n",
    "- Maintain a helpful and informative tone.\n",
    "\n",
    "<RETRIEVED_DOCUMENTS>\n",
    "{context}\n",
    "</RETRIEVED_DOCUMENTS>\n",
    "\n",
    "User Question: {query}\n",
    "\n",
    "Financial Literacy Assistant's Answer:\n",
    "\"\"\"\n",
    "    logging.info(\"Sending query to LLM...\")\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=llm_model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=temperature, # Adjusted from 0.2 to 0.5 for a bit more flexibility\n",
    "            max_tokens=700, # Increased max_tokens to allow for more comprehensive answers\n",
    "        )\n",
    "        answer = response.choices[0].message.content.strip()\n",
    "        logging.info(\"LLM response received.\")\n",
    "        return answer\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error calling LLM API: {e}\")\n",
    "        return \"I'm sorry, I'm having trouble processing your request right now. Please try again later.\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Welcome to Financial Literacy Chatbot (short pipeline demo). Type 'exit' to quit.\")\n",
    "    # Add a check to ensure chunks and embeddings are loaded before starting interaction\n",
    "    if not chunks or not chunk_embeddings:\n",
    "        print(\"Chatbot cannot start due to missing data. Please check logs for errors.\")\n",
    "    else:\n",
    "        while True:\n",
    "            query = input(\"\\nAsk your question: \")\n",
    "            if query.lower() == \"exit\":\n",
    "                break\n",
    "            answer = answer_query(query)\n",
    "            print(f\"\\nðŸ’¬ Answer:\\n{answer}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2516f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
